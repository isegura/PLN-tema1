{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isegura/PLN-tema1/blob/main/practica_nltk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfuy_vafYI6O"
      },
      "source": [
        "# Práctica: Fundamentos de PLN con NLTK\n",
        "\n",
        "**Asignatura:** Procesamiento de Lenguaje Natural con Aprendizaje Profundo (UC3M)\n",
        "\n",
        "En esta práctica usarás **NLTK** para implementar tareas típicas de PLN (preprocesado, análisis léxico, n-gramas y recursos semánticos).  \n",
        "El objetivo es entender *qué hacen* estas herramientas y cómo se integran en un flujo de trabajo.\n",
        "\n",
        "✅ **Entrega sugerida**: sube este notebook completado (celdas con `TODO`).\n"
      ],
      "id": "nfuy_vafYI6O"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf6LvrVSYI6Q"
      },
      "source": [
        "## 0. Preparación del entorno\n",
        "Ejecuta la instalación solo si NLTK no está disponible en tu entorno. En entornos con restricciones, consulta a la profesora.\n"
      ],
      "id": "Wf6LvrVSYI6Q"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXj6G5zbYI6R",
        "outputId": "4934c2a7-92c4-4e2c-f9b2-2f1c5fa2f58a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK version: 3.9.1\n"
          ]
        }
      ],
      "source": [
        "# Si lo necesitas, descomenta:\n",
        "# !pip install -q nltk\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "print('NLTK version:', nltk.__version__)"
      ],
      "id": "TXj6G5zbYI6R"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFSzvLW6YI6S"
      },
      "source": [
        "### Descarga de recursos\n",
        "NLTK separa el código de los **recursos** (corpora, modelos, etc.). Descarga solo lo necesario.\n"
      ],
      "id": "ZFSzvLW6YI6S"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a795JSq8YI6S",
        "outputId": "536f64c9-e1f3-48e8-e31b-62f1826e84fd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "downloads = [\n",
        "    'punkt_tab',          # tokenización\n",
        "    'stopwords',      # stopwords\n",
        "    'wordnet',        # WordNet\n",
        "    'omw-1.4',        # WordNet multilingüe\n",
        "    'averaged_perceptron_tagger_eng',  # POS tagger (inglés)\n",
        "    'maxent_ne_chunker_tab',\n",
        "    'words',\n",
        "]\n",
        "for pkg in downloads:\n",
        "    try:\n",
        "        nltk.data.find(pkg)\n",
        "    except LookupError:\n",
        "        nltk.download(pkg)"
      ],
      "id": "a795JSq8YI6S"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ODGE56PYI6S"
      },
      "source": [
        "## 1. Dataset de juguete (texto en español)\n",
        "Usaremos un pequeño conjunto de frases para practicar. Puedes ampliarlo con noticias, reseñas o textos de tu interés.\n"
      ],
      "id": "-ODGE56PYI6S"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SK8-j78ZYI6T",
        "outputId": "987731e9-57bf-4e87-8cf7-b7c65a13c5cd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nº documentos: 5\n",
            "Me encanta el PLN, pero a veces el preprocesamiento es tedioso.\n"
          ]
        }
      ],
      "source": [
        "texts_es = [\n",
        "    \"Me encanta el PLN, pero a veces el preprocesamiento es tedioso.\",\n",
        "    \"Hoy he leído un artículo interesante sobre modelos de lenguaje.\",\n",
        "    \"La tokenización y la lematización son pasos comunes en PLN.\",\n",
        "    \"Los transformers han cambiado muchas tareas de procesamiento del lenguaje.\",\n",
        "    \"SpaCy y NLTK son herramientas útiles, aunque con objetivos distintos.\",\n",
        "]\n",
        "\n",
        "print('Nº documentos:', len(texts_es))\n",
        "print(texts_es[0])"
      ],
      "id": "SK8-j78ZYI6T"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etOTMO-NYI6T"
      },
      "source": [
        "## 2. Tokenización\n",
        "La tokenización separa el texto en unidades (tokens). En NLTK hay tokenizadores por defecto.\n",
        "\n",
        "### 2.1 Tokenización a nivel de frase y palabra\n",
        "**Tarea:** tokeniza el primer texto en palabras y cuenta cuántos tokens produce.\n"
      ],
      "id": "etOTMO-NYI6T"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8JLSL1xYI6T",
        "outputId": "d5afa644-94db-4c9b-a2db-0ff1397c8d07"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Me encanta el PLN, pero a veces el preprocesamiento es tedioso.\n",
            "['Me', 'encanta', 'el', 'PLN', ',', 'pero', 'a', 'veces', 'el', 'preprocesamiento', 'es', 'tedioso', '.']\n",
            "Nº tokens: 13\n",
            "Frases: ['Me encanta el PLN, pero a veces el preprocesamiento es tedioso.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "print(texts_es[0])\n",
        "doc0 = texts_es[0]\n",
        "\n",
        "# TODO: tokeniza doc0 en palabras usando word_tokenize\n",
        "tokens0 = word_tokenize(doc0, language='spanish')\n",
        "print(tokens0)\n",
        "print('Nº tokens:', len(tokens0))\n",
        "\n",
        "# Extra: tokenización por frases (si hay varias)\n",
        "print('Frases:', sent_tokenize(doc0, language='spanish'))"
      ],
      "id": "G8JLSL1xYI6T"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmNyhmokYI6U"
      },
      "source": [
        "## 3. Normalización y limpieza\n",
        "En pipelines reales suele normalizarse el texto: minúsculas, eliminación de puntuación, números, etc.\n",
        "\n",
        "### 3.1 Función de preprocesado\n",
        "**Tarea:** implementa `preprocess()` que:\n",
        "1) pase a minúsculas, 2) elimine tokens que no sean letras (p. ej. puntuación), 3) elimine stopwords en español.\n"
      ],
      "id": "QmNyhmokYI6U"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6NRtO9hYI6U",
        "outputId": "6bc9949d-1813-4ab1-9cca-ce579d35b01a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['encanta', 'pln', 'veces', 'preprocesamiento', 'tedioso']\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_es = set(stopwords.words('spanish'))\n",
        "\n",
        "def preprocess(text):\n",
        "    \"\"\"Devuelve una lista de tokens limpios. Sin stopwords.\n",
        "    Sin números. Sin signos de puntuación\"\"\"\n",
        "    tokens = word_tokenize(text.lower(), language='spanish')\n",
        "    # únicamente conservamos los tokens que sean palabras; eliminamos números y signos de puntuación, etc\n",
        "    tokens = [t for t in tokens if re.fullmatch(r\"[a-záéíóúüñ]+\", t)]\n",
        "    tokens = [t for t in tokens if t not in stop_es]\n",
        "    return tokens\n",
        "\n",
        "print(preprocess(texts_es[0]))"
      ],
      "id": "e6NRtO9hYI6U"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_w_qqm6YI6U"
      },
      "source": [
        "## 4. Frecuencias y n-gramas\n",
        "Vamos a calcular frecuencias y bigramas para ver patrones léxicos.\n",
        "\n",
        "### 4.1 Frecuencia de tokens\n",
        "**Tarea:** calcula el top-10 de tokens más frecuentes del corpus.\n"
      ],
      "id": "o_w_qqm6YI6U"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKkN3J1NYI6U",
        "outputId": "6cd0b9db-4c47-4080-a45e-fe854107a6db"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-10 tokens: [('pln', 2), ('lenguaje', 2), ('encanta', 1), ('veces', 1), ('preprocesamiento', 1), ('tedioso', 1), ('hoy', 1), ('leído', 1), ('artículo', 1), ('interesante', 1)]\n"
          ]
        }
      ],
      "source": [
        "corpus_tokens = []\n",
        "for t in texts_es:\n",
        "    corpus_tokens.extend(preprocess(t))\n",
        "\n",
        "freq = Counter(corpus_tokens)\n",
        "print('Top-10 tokens:', freq.most_common(10))"
      ],
      "id": "LKkN3J1NYI6U"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivD77TdPYI6U"
      },
      "source": [
        "### 4.2 Bigrama más frecuente\n",
        "**Tarea:** calcula los bigramas sobre los tokens del corpus y muestra los 10 más frecuentes.\n"
      ],
      "id": "ivD77TdPYI6U"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VoNOrbLYI6U",
        "outputId": "1d4a685d-dd3c-4943-fc05-0050cbfcac68"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-10 bigramas: [(('encanta', 'pln'), 1), (('pln', 'veces'), 1), (('veces', 'preprocesamiento'), 1), (('preprocesamiento', 'tedioso'), 1), (('tedioso', 'hoy'), 1), (('hoy', 'leído'), 1), (('leído', 'artículo'), 1), (('artículo', 'interesante'), 1), (('interesante', 'modelos'), 1), (('modelos', 'lenguaje'), 1)]\n"
          ]
        }
      ],
      "source": [
        "from nltk import bigrams\n",
        "\n",
        "bi = list(bigrams(corpus_tokens))\n",
        "bi_freq = Counter(bi)\n",
        "print('Top-10 bigramas:', bi_freq.most_common(10))"
      ],
      "id": "0VoNOrbLYI6U"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZCF2Y3WYI6U"
      },
      "source": [
        "## 5. Stemming (raíz) en español\n",
        "El **stemming** reduce palabras a una raíz aproximada. No es una lematización: puede producir formas no lingüísticas.\n",
        "\n",
        "**Tarea:** aplica un stemmer en español y compara tokens originales vs stems.\n"
      ],
      "id": "qZCF2Y3WYI6U"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuM_4Cj3YI6U",
        "outputId": "24eeec70-3bb2-45ea-d315-2e7ded3d3d56"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens: ['modelos', 'lenguaje', 'generan', 'textos', 'interesantes']\n",
            "stems : ['model', 'lenguaj', 'gener', 'text', 'interes']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "stemmer_es = SnowballStemmer('spanish')\n",
        "\n",
        "example = preprocess(\"Los modelos de lenguaje generan textos interesantes\")\n",
        "stems = [stemmer_es.stem(w) for w in example]\n",
        "print('tokens:', example)\n",
        "print('stems :', stems)"
      ],
      "id": "LuM_4Cj3YI6U"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xp8-aKxyYI6V"
      },
      "source": [
        "## 6. Semántica léxica con WordNet (ejemplo en inglés)\n",
        "WordNet es un recurso semántico principalmente en inglés. Aquí lo usamos para explorar **sinónimos** y **relaciones**.\n",
        "\n",
        "**Tarea:** para la palabra `bank`, muestra 2 synsets y sus definiciones. ¿Qué ambigüedad observas?\n"
      ],
      "id": "xp8-aKxyYI6V"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCnnImaXYI6V",
        "outputId": "49cdf4bd-10bc-4cf6-f4ab-28c9b476238c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nº synsets: 18\n",
            "- bank.n.01 -> sloping land (especially the slope beside a body of water)\n",
            "- depository_financial_institution.n.01 -> a financial institution that accepts deposits and channels the money into lending activities\n",
            "Lemas (sinónimos aproximados): ['bank']\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "word = 'bank'\n",
        "synsets = wn.synsets(word)\n",
        "print('Nº synsets:', len(synsets))\n",
        "for s in synsets[:2]:\n",
        "    print('-', s.name(), '->', s.definition())\n",
        "\n",
        "# Extra: sinónimos del primer synset\n",
        "lemmas = synsets[0].lemmas()\n",
        "print('Lemas (sinónimos aproximados):', [l.name() for l in lemmas])"
      ],
      "id": "BCnnImaXYI6V"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ohub7pTRYI6V"
      },
      "source": [
        "## 7. POS tagging (etiquetado morfosintáctico) (inglés)\n",
        "NLTK incluye etiquetadores POS entrenados (principalmente para inglés). Esto es útil para entender el concepto.\n",
        "\n",
        "**Tarea:** etiqueta el texto y calcula cuántos sustantivos (NN, NNS, NNP, NNPS) hay.\n"
      ],
      "id": "Ohub7pTRYI6V"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ti8y32w-YI6V",
        "outputId": "2f763337-f492-49b7-f264-d8e6d87c4357"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Transformers', 'NNS'), ('changed', 'VBD'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('and', 'CC'), ('enabled', 'VBD'), ('new', 'JJ'), ('applications', 'NNS'), ('.', '.')]\n",
            "Nº sustantivos: 4\n"
          ]
        }
      ],
      "source": [
        "from nltk import pos_tag\n",
        "\n",
        "text_en = \"Transformers changed natural language processing and enabled new applications.\"\n",
        "tokens_en = word_tokenize(text_en)\n",
        "tags = pos_tag(tokens_en)\n",
        "print(tags)\n",
        "\n",
        "# TODO: cuenta sustantivos\n",
        "noun_tags = {'NN', 'NNS', 'NNP', 'NNPS'}\n",
        "n_nouns = sum(1 for _, tag in tags if tag in noun_tags)\n",
        "print('Nº sustantivos:', n_nouns)"
      ],
      "id": "ti8y32w-YI6V"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TtZCEaNYI6V"
      },
      "source": [
        "## 8. NER con chunking (inglés)\n",
        "Ejemplo rápido de reconocimiento de entidades con el chunker de NLTK.\n",
        "\n",
        "**Tarea:** ejecuta NER y observa qué entidades detecta.\n"
      ],
      "id": "1TtZCEaNYI6V"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIkDbRYgYI6V",
        "outputId": "f9aea20b-c11b-422b-aa54-6fbbd5f5fda4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON Barack/NNP)\n",
            "  (PERSON Obama/NNP)\n",
            "  visited/VBD\n",
            "  (GPE Madrid/NNP)\n",
            "  in/IN\n",
            "  2016/CD\n",
            "  and/CC\n",
            "  met/VBD\n",
            "  (GPE Google/NNP)\n",
            "  researchers/NNS\n",
            "  ./.)\n",
            "Entidades: [('Barack', 'PERSON'), ('Obama', 'PERSON'), ('Madrid', 'GPE'), ('Google', 'GPE')]\n"
          ]
        }
      ],
      "source": [
        "from nltk import ne_chunk\n",
        "\n",
        "text_en2 = \"Barack Obama visited Madrid in 2016 and met Google researchers.\"\n",
        "tokens2 = word_tokenize(text_en2)\n",
        "tags2 = pos_tag(tokens2)\n",
        "tree = ne_chunk(tags2)\n",
        "print(tree)\n",
        "\n",
        "# Extra: extraer entidades\n",
        "entities = []\n",
        "for subtree in tree:\n",
        "    if hasattr(subtree, 'label'):\n",
        "        ent = ' '.join([t for t, _ in subtree.leaves()])\n",
        "        entities.append((ent, subtree.label()))\n",
        "print('Entidades:', entities)"
      ],
      "id": "RIkDbRYgYI6V"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmJDkQqyYI6V"
      },
      "source": [
        "### Ejercicio 1: Clasificación simple basada en reglas\n",
        "- Define una lista de palabras positivas y negativas (en español).\n",
        "- Implementa una función que etiquete cada oración como `POS`, `NEG` o `NEU` según la frecuecia de palabras positivas y negativas. Si el número de palabras positivas y negativas es idéntico, el texto se etiqueta como 'NEU'.\n",
        "- Evalúa sobre al menos 15 oraciones.\n",
        "\n",
        "### Ejercicio 2: Comparativa stemming vs (pseudo)lematización\n",
        "- Selecciona 20 palabras (inglés) y compara stemming vs lematización.\n",
        "- Discute qué errores introduce cada enfoque.\n"
      ],
      "id": "AmJDkQqyYI6V"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QE5s8fQxYI6W"
      },
      "source": [
        "---\n",
        "### Preguntas de reflexión (para discutir)\n",
        "1. ¿Qué errores puede introducir la tokenización?\n",
        "2. ¿Cuándo es útil eliminar stopwords y cuándo puede ser perjudicial?\n",
        "3. ¿Qué diferencia observas entre stemming y lematización?\n",
        "4. ¿Por qué WordNet no es suficiente para capturar semántica contextual?\n"
      ],
      "id": "QE5s8fQxYI6W"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}