{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLi9qvYRdcX+ud1NkGZdAq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isegura/PLN-tema1/blob/main/3_lematizacion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Normalization\n",
        "El objetivo es reducir la variabilidad del lenguaje. Ejemplo:\n",
        "\n",
        "- Lisa comió la comida y lavó los platos.\n",
        "- Estaban comiendo fideos en un café.\n",
        "- ¿No quieres comer antes de que nos vayamos?\n",
        "- Comeré el sábado con mis amigos.\n",
        "- También come frutas y verduras.\n",
        "\n",
        "En el desarrollo de muchos sistemas de PLN, es necesario normalizar las palabras (convertirlas a su raíz o a su forma canónica) para evitar su posible variabilidad (distintas formas, pero igual significado semántico).\n",
        "\n",
        "La normalización no sólo permite tratar la variabilidad, sino también disminuir el tamaño del vocabulario.\n",
        "\n",
        "Dos tipos de normalización:\n",
        "\n",
        "- steming, que consiste transformar la palabra a su raíz.\n",
        "- lematización, que es transformar una palabra a su lema o forma canónica.\n",
        "\n",
        "Stemming suele ser un proceso sencillo y rápido, basado en un conjunto de reglas que permite cortar la palabra a partir de un conjunto de prefijos y sufijos (por ejemplo, “ing”, “ly”, “es”, “s”).\n",
        "\n",
        "La lematización es más lento pero la información que devuelve es de mayor calidad ya que utiliza diccionarios con detallado conocimiento lingüístico."
      ],
      "metadata": {
        "id": "deRTVUZ-9zXg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lematización con nltk\n"
      ],
      "metadata": {
        "id": "ivOGtrLH-WsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udr8c8PU-dlO",
        "outputId": "aa632b78-4521-409e-b138-fd143a52bb85"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1Qb1uNR9tQY",
        "outputId": "f71c594d-8395-4b95-c334-1737f03bc6bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'children', 'were', 'playing', ',', 'while', 'their', 'parents', 'were', 'chatting', '.']\n",
            "['the', 'child', 'were', 'playing', ',', 'while', 'their', 'parent', 'were', 'chatting', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "text='The children were playing, while their parents were chatting.'\n",
        "text=text.lower()\n",
        "\n",
        "word_tokens = word_tokenize(text)\n",
        "\n",
        "filtered_tokens = []\n",
        "\n",
        "for w in word_tokens:\n",
        "    filtered_tokens.append(w)\n",
        "print(filtered_tokens)\n",
        "\n",
        "lemma_word = []\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "for w in filtered_tokens:\n",
        "    word1 = wordnet_lemmatizer.lemmatize(w, pos = \"n\")\n",
        "    word2 = wordnet_lemmatizer.lemmatize(word1, pos = \"v\")\n",
        "    word3 = wordnet_lemmatizer.lemmatize(word2, pos = (\"a\"))\n",
        "    # print(w, word1, word2, word2)\n",
        "    lemma_word.append(word1)\n",
        "\n",
        "print(lemma_word)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lematización con spacy\n"
      ],
      "metadata": {
        "id": "fGKat37r-RR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#make sure to download the english model with \"python -m spacy download en\"\n",
        "\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "text='The children were playing, while their parents were chatting.'\n",
        "text=text.lower()\n",
        "doc = nlp(text)\n",
        "\n",
        "lemmas = []\n",
        "tokens=[]\n",
        "for token in doc:\n",
        "    lemmas.append(token.lemma_)\n",
        "    tokens.append(token)\n",
        "print(tokens)\n",
        "print(lemmas)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfyJigf_-QJN",
        "outputId": "0afd6dfb-f98a-4a98-a6f5-d4156cb7e6d7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[the, children, were, playing, ,, while, their, parents, were, chatting, .]\n",
            "['the', 'child', 'be', 'play', ',', 'while', 'their', 'parent', 'be', 'chat', '.']\n"
          ]
        }
      ]
    }
  ]
}