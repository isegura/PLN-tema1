{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isegura/PLN-tema1/blob/main/2_stopwords.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Gm0kEk15uOd"
      },
      "source": [
        "# Stopwords\n",
        "\n",
        "\n",
        "Las palabras vacías son las palabras más comunes de un idioma.\n",
        "\n",
        "Por ejemplo, las palabras más comunes en inglés: **“the”, “is”, “in”, “for”, “where”, “when”, “to”, “at”** etc.\n",
        "\n",
        "Las palabras más comunes en español son: **de, que, el, en y, a ,los, se, del, las, un, por, con, no una**, etc.\n",
        "\n",
        " Estas palabras no suelen agregar significado a un documento, por este motivo suelen ser eliminadas del textos a la hora de construir sistemas de PLN. En especial, suelen ser eliminadas en las tareas de clasificación de textos para poder centrarnos en las palabras que aportan mayor significado al texto y que dan más información para su clasificación.\n",
        "\n",
        " ¿Qué ventajas tiene eliminar las stopwords?\n",
        " - El tamaño del dataset disminuye, y por tanto, el tiempo para entrenar el model también.\n",
        " - En tareas como la clasificación de textos o la recuperación de información, los resultados suelen mejorar porque en el dataset sólo quedan palabras que dan significado al texto.\n",
        "\n",
        " Sin embargo, hay otras tareas de PLN como la traducción automática o la simplificación donde no es una buena idea eliminar las stopwords.\n",
        "\n",
        " Existen varias librerías de Python que nos permiten eliminar las stopwords finalmente.\n",
        "\n",
        " ## Eliminando stopwords (con NLTK):\n",
        "\n",
        " En primer lugar, vemos un ejemplo con NLTK:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "933UmmKw8SiI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbe05f30-ee0c-4791-df7d-c52151acf6d1"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "# Mostramos sólo las 10 primeras\n",
        "list(set(stopwords.words('english')))[:10]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['how',\n",
              " 'when',\n",
              " 'ain',\n",
              " 'does',\n",
              " \"hadn't\",\n",
              " \"you've\",\n",
              " \"didn't\",\n",
              " \"we're\",\n",
              " 'for',\n",
              " 'other']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cF2QXtCK8t0K",
        "outputId": "74b80046-d4c6-44e7-b0fd-c758f4c322ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "text='The children were playing, while their parents were chatting.'\n",
        "text=text.lower()\n",
        "\n",
        "# set of stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# tokens of words\n",
        "word_tokens = word_tokenize(text)\n",
        "\n",
        "filtered = []\n",
        "\n",
        "for w in word_tokens:\n",
        "    if w not in stop_words:\n",
        "        filtered.append(w)\n",
        "\n",
        "print(\"\\n\\nOriginal Sentence \\n\")\n",
        "print(\" \".join(word_tokens))\n",
        "\n",
        "print(\"\\n\\nText without stopwords \\n\")\n",
        "print(\" \".join(filtered))\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Original Sentence \n",
            "\n",
            "the children were playing , while their parents were chatting .\n",
            "\n",
            "\n",
            "Text without stopwords \n",
            "\n",
            "children playing , parents chatting .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ql-v9QrCpd7"
      },
      "source": [
        "## Eliminando stopwords con Spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzGHXwOUCsg_",
        "outputId": "4a066e2e-6bc3-49ad-c5b1-139254ec8660",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from spacy.lang.en import English\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "nlp = English()\n",
        "\n",
        "text='The children were playing, while their parents were chatting.'\n",
        "text=text.lower()\n",
        "\n",
        "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
        "my_doc = nlp(text)\n",
        "\n",
        "# Create list of word tokens\n",
        "token_list = []\n",
        "for token in my_doc:\n",
        "    token_list.append(token.text)\n",
        "\n",
        "\n",
        "# Create list of word tokens after removing stopwords\n",
        "filtered =[]\n",
        "\n",
        "for word in token_list:\n",
        "    lexeme = nlp.vocab[word]\n",
        "    if lexeme.is_stop == False:\n",
        "        filtered.append(word)\n",
        "\n",
        "print(\"\\n\\nOriginal Sentence \\n\")\n",
        "print(\" \".join(token_list))\n",
        "\n",
        "print(\"\\n\\nText without stopwords \\n\")\n",
        "print(\" \".join(filtered))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Original Sentence \n",
            "\n",
            "the children were playing , while their parents were chatting .\n",
            "\n",
            "\n",
            "Text without stopwords \n",
            "\n",
            "children playing , parents chatting .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicio: Eliminar Stopwords para español usando Spacy\n",
        "\n",
        "Implementa un código que procese una oración en Español y elimine sus stopwords."
      ],
      "metadata": {
        "id": "Yn_n9g8B63Q3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download es_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPCtruaA7pnU",
        "outputId": "66b78575-d124-473e-88d1-22373c99a83e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting es-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "text = \"Los niños estaban jugando en el parque por la tarde.\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "# Eliminar stop words y puntuación\n",
        "tokens_sin_stopwords = [\n",
        "    token.text\n",
        "    for token in doc\n",
        "    if not token.is_stop and not token.is_punct\n",
        "]\n",
        "\n",
        "print(tokens_sin_stopwords)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3N9SnuH7HUC",
        "outputId": "f87db40b-f893-4cca-cb06-e2801ce9c087"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['niños', 'jugando', 'parque']\n"
          ]
        }
      ]
    }
  ]
}